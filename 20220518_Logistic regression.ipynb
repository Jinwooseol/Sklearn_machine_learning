{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20220518_Logistic regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOVPZlTTd/4tsLu5XC0O+Y3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"H0cCcTzdmwwH","executionInfo":{"status":"ok","timestamp":1653571455777,"user_tz":-540,"elapsed":1962,"user":{"displayName":"설진우","userId":"17367759130491423653"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import datasets\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, SGDClassifier\n","from sklearn.metrics import r2_score, mean_squared_error\n","from sklearn.model_selection import train_test_split, KFold\n","import math"]},{"cell_type":"markdown","source":["# Logistic Regression\n","- Linear regression을 통해 얻은 y_hat (prediction value)를 사용해서 특정 **class에 포함될 확률**이 얼마인지를 추정한다.\n","- **Sigmoid function**을 사용한다.\n","- 해당 확률을 이용하여 **classification**에 사용한다."],"metadata":{"id":"oSCAsnAQvfGj"}},{"cell_type":"markdown","source":["$p:$ 해당 class일 확률  \n","$1 - p:$ 해당 class가 아닐 확률\n","\n","odd_ratio = $p / (1 - p)$  \n","logit(function), z = $log$(odd_ratio) = $log(p/(1-p))$  \n","sigmoid, p = $1 / (1 + e^{-z})$  "],"metadata":{"id":"22e8_Gz7wKVY"}},{"cell_type":"markdown","source":["# LinearRegression\n","- **y_hat** = $a*x + b$\n","- **Loss of function** = SSE, MSE\n","\n","# LogisticRegression\n","- **z** = sigmoid($a*x + b$)\n","- **Loss of function** = Log Loss\n","- $-ylog{z} - (1-y)log{(1-z)}$\n","  - $y$: Target value (0, 1)  \n","    (Class에 속하거나 아니거나)\n","  - $z$: Predict value\n","\n","- $y == 0$:  \n","  $-log(1-z)$\n","- $y == 1$:  \n","  $-log(z)$\n","\n","- **Penalty**: 'none', 'l1', 'l2'(default), 'elasticnet'  \n","  ('l1': Lasso regulation, 'l2': Ridge regulation, 'elasticnet': Both regulation)\n","\n","- **Solver**: ‘newton-cg’, ‘lbfgs’(default), ‘liblinear’, ‘sag’, ‘saga’  \n","  \n","\n","# Cross_entropy\n","- Log Loss보다 더 발전한 형태의 loss of function"],"metadata":{"id":"2_T1jfdKxtaQ"}},{"cell_type":"markdown","source":["# Import Iris data"],"metadata":{"id":"74hVUNDw1Jb9"}},{"cell_type":"code","source":["iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target     # class 값 (0, 1, 2)\n","\n","print(iris.DESCR)\n","print(y)\n","\n","#X_train, X_test, y_train, y_test = train_test_split(X, y)   # stratify\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)   # stratify"],"metadata":{"id":"x4gc8s1NvFhw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653572785711,"user_tz":-540,"elapsed":255,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"32ebfd8c-a8c4-4323-c704-5c64674a56fe"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, predictive attributes and the class\n","    :Attribute Information:\n","        - sepal length in cm\n","        - sepal width in cm\n","        - petal length in cm\n","        - petal width in cm\n","        - class:\n","                - Iris-Setosa\n","                - Iris-Versicolour\n","                - Iris-Virginica\n","                \n","    :Summary Statistics:\n","\n","    ============== ==== ==== ======= ===== ====================\n","                    Min  Max   Mean    SD   Class Correlation\n","    ============== ==== ==== ======= ===== ====================\n","    sepal length:   4.3  7.9   5.84   0.83    0.7826\n","    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n","    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n","    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n","    ============== ==== ==== ======= ===== ====================\n","\n","    :Missing Attribute Values: None\n","    :Class Distribution: 33.3% for each of 3 classes.\n","    :Creator: R.A. Fisher\n","    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n","    :Date: July, 1988\n","\n","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n","from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n","Machine Learning Repository, which has two wrong data points.\n","\n","This is perhaps the best known database to be found in the\n","pattern recognition literature.  Fisher's paper is a classic in the field and\n","is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n","data set contains 3 classes of 50 instances each, where each class refers to a\n","type of iris plant.  One class is linearly separable from the other 2; the\n","latter are NOT linearly separable from each other.\n","\n",".. topic:: References\n","\n","   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n","     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n","     Mathematical Statistics\" (John Wiley, NY, 1950).\n","   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n","     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n","   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n","     Structure and Classification Rule for Recognition in Partially Exposed\n","     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n","     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n","   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n","     on Information Theory, May 1972, 431-433.\n","   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n","     conceptual clustering system finds 3 classes in the data.\n","   - Many, many more ...\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n"]}]},{"cell_type":"markdown","source":["# Stratify\n","- 각 class마다 같은 비율로 training, testing data를 분리한다.\n","- stratify를 설정하지 않을 경우, class마다 training, testing dataset의 비율에 있어서 차이가 존재한다."],"metadata":{"id":"cr2wGvheEvBV"}},{"cell_type":"markdown","source":["- coef_, intercept_ 의 set은 target의 수(class의 수)에 따라 결정된다.\n","\n","\n","# Class_0 일 확률을 내는 Model (Example)\n","- z = $-0.448*a + 0.860*b -2.370*c -1.008*d + 9.622$  \n","  (decision_function)"],"metadata":{"id":"tHgGR7Ic2yk2"}},{"cell_type":"code","source":["l1 = LogisticRegression(solver = 'sag').fit(X_train, y_train)\n","print(l1.score(X_test, y_test))\n","print(l1.coef_)\n","print(l1.intercept_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnQrwI5E1hqj","executionInfo":{"status":"ok","timestamp":1653572039285,"user_tz":-540,"elapsed":263,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"90cd6704-fad9-40e3-a38c-a274c75eb941"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9736842105263158\n","[[ 0.57416299  1.45268554 -2.17534785 -0.98715642]\n"," [ 0.27967105 -0.15925862  0.09468578 -0.9893571 ]\n"," [-0.85383404 -1.29342692  2.08066206  1.97651352]]\n","[ 1.37059634  1.53846864 -2.90906499]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n"]}]},{"cell_type":"markdown","source":["- Stochastic Average Gradient (SAG) algorithm이 가장 score가 높게 나왔다.\n","- 확률이 가장 높은 class로 결정한다.\n","- decision_function의 결과값이 y_hat 값을 sigmoid function에 넣은 값이다.\n","- 따라서, decision_function이 가장 높은 class로 결정한다."],"metadata":{"id":"1ved6Drp36fX"}},{"cell_type":"code","source":["d = l1.decision_function(X_test)\n","p = l1.predict_proba(X_test)  # probability\n","print(d[0])\n","\n","print(np.dot(X_test[0], l1.coef_[0]) + l1.intercept_[0])    # class 0일 가능성 점수\n","print(np.dot(X_test[0], l1.coef_[1]) + l1.intercept_[1])    # class 1일 가능성 점수\n","print(np.dot(X_test[0], l1.coef_[2]) + l1.intercept_[2])    # class 2일 가능성 점수\n","\n","print(mysig(d[0]))\n","print(p[0])\n","\n","print('Predict: ',np.argmax(p, axis = 1))  # 가장 높은 확률을 가진 class를 도출   # 예측 값\n","print('Predict: ',l1.predict(X_test))                                             # 예측 값\n","print('Target: ',y_test)                                                         # 실제 정답"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zsTjlxMM16nH","executionInfo":{"status":"ok","timestamp":1653572792037,"user_tz":-540,"elapsed":280,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"296eca11-88b5-4a9b-c067-6089747e9b79"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 4.90102032  2.10552481 -7.00654513]\n","4.901020323322449\n","2.1055248089547858\n","-7.006545132277051\n","[9.92615941e-01 8.91439002e-01 9.05113095e-04]\n","[9.42425939e-01 5.75677101e-02 6.35122152e-06]\n","Predict:  [0 2 0 2 2 0 2 2 2 0 0 2 1 0 0 0 1 1 2 2 1 2 2 0 1 0 1 0 1 2 1 1 0 1 1 2 2\n"," 0]\n","Predict:  [0 2 0 2 2 0 2 2 2 0 0 2 1 0 0 0 1 1 2 2 1 2 2 0 1 0 1 0 1 2 1 1 0 1 1 2 2\n"," 0]\n","Target:  [0 2 0 2 2 0 2 2 2 0 0 2 1 0 0 0 1 1 1 2 1 2 2 0 1 0 1 0 1 2 1 1 0 1 1 2 2\n"," 0]\n"]}]},{"cell_type":"markdown","source":["# Sigmoid function"],"metadata":{"id":"AILMs05S4dp4"}},{"cell_type":"code","source":["def mysig(z):\n","  return 1/(1+np.exp(-z))\n"],"metadata":{"id":"jXgvpbra3p2K","executionInfo":{"status":"ok","timestamp":1653572758664,"user_tz":-540,"elapsed":293,"user":{"displayName":"설진우","userId":"17367759130491423653"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Stochastic Gradient Descent Classifier (SGDClassifier)\n","- Logistic Regression의 하위버전 (범용)\n","- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"],"metadata":{"id":"NMRFu1t2-MQz"}},{"cell_type":"code","source":["l2 = SGDClassifier(loss = 'log')  # loss = 'log' -> log loss, loss of function of logistic regression\n","l2.fit(X_train, y_train)\n","l2.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sKOPyUY4siY","executionInfo":{"status":"ok","timestamp":1653572760632,"user_tz":-540,"elapsed":306,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"5b14c496-a628-4e76-e3f1-1c81c6b51c30"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9473684210526315"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Application\n","- Wine의 특징을 통해 wine의 class 분류하기\n","- independent variables: Alcohol, color intensity, Hue\n","- Target: class\n","\n","- Logistic Regression or SGDClassifier"],"metadata":{"id":"HDBrTskJGDqq"}},{"cell_type":"code","source":["wine = datasets.load_wine(as_frame = True)\n","print(wine.data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hr4vIwd9-r5V","executionInfo":{"status":"ok","timestamp":1653572763444,"user_tz":-540,"elapsed":5,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"3bd1124e-ff94-4fa3-915d-1e18a52b9c45"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n","0      14.23        1.71  2.43               15.6      127.0           2.80   \n","1      13.20        1.78  2.14               11.2      100.0           2.65   \n","2      13.16        2.36  2.67               18.6      101.0           2.80   \n","3      14.37        1.95  2.50               16.8      113.0           3.85   \n","4      13.24        2.59  2.87               21.0      118.0           2.80   \n","..       ...         ...   ...                ...        ...            ...   \n","173    13.71        5.65  2.45               20.5       95.0           1.68   \n","174    13.40        3.91  2.48               23.0      102.0           1.80   \n","175    13.27        4.28  2.26               20.0      120.0           1.59   \n","176    13.17        2.59  2.37               20.0      120.0           1.65   \n","177    14.13        4.10  2.74               24.5       96.0           2.05   \n","\n","     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n","0          3.06                  0.28             2.29             5.64  1.04   \n","1          2.76                  0.26             1.28             4.38  1.05   \n","2          3.24                  0.30             2.81             5.68  1.03   \n","3          3.49                  0.24             2.18             7.80  0.86   \n","4          2.69                  0.39             1.82             4.32  1.04   \n","..          ...                   ...              ...              ...   ...   \n","173        0.61                  0.52             1.06             7.70  0.64   \n","174        0.75                  0.43             1.41             7.30  0.70   \n","175        0.69                  0.43             1.35            10.20  0.59   \n","176        0.68                  0.53             1.46             9.30  0.60   \n","177        0.76                  0.56             1.35             9.20  0.61   \n","\n","     od280/od315_of_diluted_wines  proline  \n","0                            3.92   1065.0  \n","1                            3.40   1050.0  \n","2                            3.17   1185.0  \n","3                            3.45   1480.0  \n","4                            2.93    735.0  \n","..                            ...      ...  \n","173                          1.74    740.0  \n","174                          1.56    750.0  \n","175                          1.56    835.0  \n","176                          1.62    840.0  \n","177                          1.60    560.0  \n","\n","[178 rows x 13 columns]\n"]}]},{"cell_type":"markdown","source":["# Classifier"],"metadata":{"id":"bLxbTiIPGRGS"}},{"cell_type":"code","source":["X, y = wine.data[['alcohol', 'color_intensity', 'hue']].values, wine.target.values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)\n","\n","wine_classifier1 = LogisticRegression(max_iter = 10000, solver = 'newton-cg').fit(X_train, y_train)\n","print('Newton-cg Score: ', wine_classifier1.score(X_test, y_test))\n","\n","wine_classifier2 = LogisticRegression(max_iter = 10000, solver = 'lbfgs').fit(X_train, y_train)\n","print('lbfgs Score: ', wine_classifier2.score(X_test, y_test))\n","\n","wine_classifier3 = LogisticRegression(max_iter = 10000, solver = 'liblinear').fit(X_train, y_train)\n","print('liblinear Score: ', wine_classifier3.score(X_test, y_test))\n","\n","wine_classifier4 = LogisticRegression(max_iter = 10000, solver = 'sag').fit(X_train, y_train)\n","print('sag Score: ', wine_classifier4.score(X_test, y_test))\n","\n","wine_classifier5 = LogisticRegression(max_iter = 10000, solver = 'saga').fit(X_train, y_train)\n","print('saga test Score: ', wine_classifier5.score(X_test, y_test))\n","print('saga train Score: ', wine_classifier5.score(X_train, y_train))\n","\n","wine_classifier6 = SGDClassifier(loss = 'log', max_iter = 10000).fit(X_train, y_train)\n","print('SGD test Score: ', wine_classifier6.score(X_test, y_test))\n","print('SGD train Score: ', wine_classifier6.score(X_train, y_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVgwdgI9GQix","executionInfo":{"status":"ok","timestamp":1653572766216,"user_tz":-540,"elapsed":704,"user":{"displayName":"설진우","userId":"17367759130491423653"}},"outputId":"e6356d34-14fa-4158-b15e-a1b6105f5d3c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Newton-cg Score:  0.8222222222222222\n","lbfgs Score:  0.8222222222222222\n","liblinear Score:  0.6888888888888889\n","sag Score:  0.8\n","saga test Score:  0.7333333333333333\n","saga train Score:  0.8796992481203008\n","SGD test Score:  0.6888888888888889\n","SGD train Score:  0.7368421052631579\n"]}]},{"cell_type":"markdown","source":["- shuffle와 stratify는 함께 사용이 불가능하다.\n","- shuffle = True를 했을 경우, training data와 testing data에 특정 class가 몰리면 학습과 예측이 제대로 이루어지지 않는다.\n"],"metadata":{"id":"mnjD5pJeKAV3"}}]}